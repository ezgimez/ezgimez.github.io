<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.png"> <title>Ezgi's Garden</title> <header> <div class=blog-name ><a href="/">Ezgi Özyılkan</a></div> <nav> <ul> <li><a href="/teaching">Teaching</a> <li><a href="/publications">Papers</a> <li><a href="/service">Service</a> <li><a href="/assets/Ozyilkan_Resume_Oct2024.pdf">Resume</a> <li><a href="/assets/Ozyilkan_CV_Oct2024.pdf">CV</a> </ul> <img src="/assets/hamburger.svg" id=menu-icon > </nav> </header> <div class=franklin-content ><h2 id=about_me ><a href="#about_me" class=header-anchor >About me</a></h2> <div class=row ><div class=container ><img class=left  id=profpic src="assets/pic_ezgi.jpeg" > <p>Hi, I am Ezgi &#40;she/they&#41;. I am a <strong>PhD candidate in Electrical and Computer Engineering</strong> at <em>New York University</em> where I&#39;m advised by <a href="https://wp.nyu.edu/elza_erkip/">Elza Erkip</a>. I hold an &#40;integrated&#41; MEng degree in Electrical Electronics Engineering from <em>Imperial College London</em>. Recent collaborators include <a href="https://balle.io/">Jona Ballé</a> and <a href="https://www.ece.cornell.edu/faculty-directory/aaron-b-wagner">Aaron B. Wagner</a> and <a href="https://profiles.imperial.ac.uk/d.gunduz">Deniz Gündüz</a>. </p> <p>I am a collaborative researcher and enjoy working with people from diverse backgrounds. My current research is driven by a passion for <strong>connecting theory and practice</strong> in compression and telecommunication problems, particularly in distributed scenarios. I leverage tools from <strong>deep learning, signal processing, data compression and information theory</strong>, <em>yielding interpretable results</em>. When I am not busy with research, I enjoy hiking and expanding my food and coffee ☕ palate.</p> <p>I am always happy to chat about topics at the intersection of information theory and deep/machine learning – feel free to drop me an email at <em>ezgi&#40;dot&#41;ozyilkan&#40;at&#41;nyu&#40;dot&#41;edu</em>&#33;</p> <p>Useful links: <a href="https://scholar.google.com/citations?hl&#61;en&amp;user&#61;MVZFqdQAAAAJ">Google Scholar</a> | <a href="https://www.linkedin.com/in/ezgi-ozyilkan-21b2ab191/">LinkedIn</a> | <a href="https://arxiv.org/a/ozyilkan_e_1.html">arXiv</a> | <a href="https://github.com/ezgimez">GitHub</a></p></div></div> <h2 id=updates ><a href="#updates" class=header-anchor >Updates</a></h2> <ul> <li><p>January 2025: Our workshop proposal titled <em>&quot;Learn to Compress and Compress to Learn&quot;</em> for <a href="https://2025.ieee-isit.org/">2025 IEEE International Symposium on Information Theory</a> has been accepted&#33; </p> <li><p>November 2024: In our recent <a href="https://openreview.net/forum?id&#61;qcM1fkFj3Y">NeurIPS&#39;24 workshop paper</a>, we discuss a few overarching <em>failure modes</em> of some popular class of neural compressors – that is, their difficulty in learning discontinuous functions. </p> <li><p>October 2024: Our recent preprint titled <a href="https://arxiv.org/abs/2405.09534v1"><em>Learning-Based Compress-and-Forward Schemes for the Relay Channel</em></a> got accepted to <a href="https://www.comsoc.org/publications/journals/ieee-jsac">IEEE Journal on Communications &#40;JSAC&#41;</a> and will appear, as part of <a href="https://www.comsoc.org/publications/journals/ieee-jsac/cfp/rethinking-information-identification-representation-and">this special issue</a>, in 2025&#33;</p> <li><p>September 2024: I presented our work titled <a href="https://arxiv.org/abs/2404.14594"><em>Neural Compress-and-Forward for the Relay Channel</em></a> at <a href="https://spawc2024.org/">SPAWC 2024</a>, in the beautiful Italian city of Lucca&#33; Here is the <a href="assets/Neural_Compress-and-Forward_SPAWC2024_poster.pdf">poster</a>. </p> <li><p>July 2024: Our <a href="https://neuralcompression.github.io/workshop24">workshop proposal</a> &#40;compression &#43; machine learning&#41; for NeurIPS 2024 has been accepted&#33; Details will follow shortly :&#41; </p> </ul> <ul> <li><p>July 2024: Our recent work titled <a href="https://arxiv.org/abs/2404.14594"><em>Neural Compress-and-Forward for the Relay Channel</em></a> got accepted to <a href="https://spawc2024.org/">IEEE International Workshop on Signal Processing Advances in Wireless Communications &#40;SPAWC&#41; 2024</a>&#33;</p> <li><p>May 2024: In our recent preprints &#40;<a href="https://arxiv.org/abs/2404.14594">&#91;1&#93;</a> and <a href="https://arxiv.org/abs/2405.09534v1">&#91;2&#93;</a>&#41;, we propose neural &quot;compress-and-forward&quot; &#40;CF&#41; schemes for the relay channel, that leverage my previous neural distributed compression work. Our proposed neural CF operates closely to the maximum achievable rate in a &quot;primitive relay channel&quot; and also yields interpretable results :&#41; </p> <li><p>April 2024: Our recent work titled <a href="https://arxiv.org/abs/2310.16961"><em>&quot;Neural Distributed Compressor Discovers Binning&quot;</em></a> got accepted to IEEE Journal on Selected Areas in Information Theory &#40;JSAIT&#41;, part of the special issue on Toby Berger. </p> </ul> <ul> <li><p>April 2024: Our recent <a href="https://arxiv.org/abs/2403.08411">preprint</a> on robust distributed lossy compression was accepted to the <a href="https://2024.ieee-isit.org/">2024 IEEE International Symposium on Information Theory</a> Workshops &#40;ISIT&#39;24 Wkshps&#41;&#33;</p> <li><p>March 2024: In our recent <a href="https://arxiv.org/abs/2403.08411">preprint</a>, we extend our <a href="https://ieeexplore.ieee.org/document/10206542">neural distributed lossy compression framework</a> to more robust/general compression settings – for example, where side information may be absent. We demonstrate that our learned compressors mimic the theoretical optimum and yield interpretable results :&#41; </p> <li><p>February 2024: Our recent survey titled <em>&quot;Distributed Compression in the Era of Machine Learning: A Review of Recent Advances&quot;</em> will appear <a href="https://ee-ciss.princeton.edu/">at the Conference on Information Sciences and Systems &#40;CISS&#39;24&#41;</a> as an invited paper&#33; Preprint is available <a href="https://arxiv.org/abs/2402.07997">here</a>.</p> </ul> <ul> <li><p>January 2024: The full program for our &#39;Learn to Compress&#39; workshop @ <a href="https://2024.ieee-isit.org/workshops">ISIT&#39;24</a> &#40;including keynote speakers and call for papers&#41; is <a href="https://learn-to-compress-workshop-isit.github.io/">out</a>.</p> </ul> <ul> <li><p>December 2023: <em>&quot;Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information&quot;</em> was accepted to the inaugural <a href="https://icmlcn2024.ieee-icmlcn.org/">2024 IEEE International Conference on Machine Learning for Communication and Networking &#40;ICMLCN&#41;</a>&#33; Preprint is available <a href="https://arxiv.org/abs/2310.04311">here</a>.</p> <li><p>November 2023: Our proposal <a href="https://2024.ieee-isit.org/workshops">&quot;Learn to Compress&quot;</a> has been accepted as a workshop at <a href="https://2024.ieee-isit.org/">ISIT 2024</a>. The proposal was put forward by <a href="https://www.ece.cornell.edu/faculty-directory/aaron-b-wagner">Aaron Wagner &#40;Cornell University&#41;</a>, <a href="https://wp.nyu.edu/elza_erkip/">Elza Erkip &#40;NYU&#41;</a> and myself. We will release more details about this workshop in December – but meanwhile, feel free to check out our <a href="https://learn-to-compress-workshop-isit.github.io/">workshop website</a>&#33; </p> <li><p>October 2023: The draft version of the journal version of our previous ISIT 2023 paper is available in <a href="https://arxiv.org/abs/2310.16961">arXiv</a>&#33; We demonstrate that the neural distributed compressor mimics the theoretical optimum for more exemplary sources :&#41; </p> <li><p>July 2023: I presented our work titled <a href="https://openreview.net/forum?id&#61;3Dq4FZJSga"><em>&quot;Neural Distributed Compressor Does Binning&quot;</em></a> at Neural Compression Workshop @ ICML&#39;23. Here are the <a href="/assets/Ozyilkan_ICML2023-workshop_final.pdf">slides</a>.</p> </ul> <ul> <li><p>July 2023: I was selected as the <em>best reviewer</em> for the <a href="https://neuralcompression.github.io/workshop23">Neural Compression Workshop @ ICML&#39;23</a>. </p> <li><p>July 2023: Our recent ISIT&#39;23 work was accepted as an <em>oral presentation</em> to <a href="https://neuralcompression.github.io/workshop23">Neural Compression Workshop @ ICML&#39;23</a>. </p> <li><p>June 2023: I presented our work titled <em>&quot;Learned Wyner–Ziv Compressors Recover Binning&quot;</em> at <a href="https://isit2023.org/">International Symposium on Information Theory &#40;ISIT&#41; 2023</a>. Here are the <a href="/assets/Ozyilkan_ISIT2023_final.pdf">slides</a>&#33;</p> <li><p>June 2023: I presented a poster about our upcoming <a href="https://arxiv.org/abs/2305.04380">ISIT&#39;23 paper</a> at <a href="https://nasit.seas.upenn.edu/home">North American School of Information Theory &#40;NASIT&#41; 2023</a>.</p> </ul> <ul> <li><p>May 2023: I presented a <a href="/assets/Ozyilkan_Simons-Institute_Poster_May2023.pdf">poster</a> titled <em>Neural Distributed Compressor Does Binning</em> at UC Berkeley Simons Institute&#39;s workshop on <a href="https://simons.berkeley.edu/talks/2023-05-24"><em>Information-Theoretic Methods for Trustworthy Machine Learning</em></a>.</p> <li><p>April 2023: <em>&quot;Learned Wyner–Ziv Compressors Recover Binning&quot;</em> was accepted to <a href="https://isit2023.org/">International Symposium on Information Theory &#40;ISIT&#41; 2023</a>. Preprint is available <a href="https://arxiv.org/abs/2305.04380">here</a>&#33;</p> </ul> <div class=page-foot > <div class=copyright > &copy; <a href="https://github.com/ezgimez">Ezgi Özyılkan</a>. Last modified: 2025-01-17. Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> + <a href="https://julialang.org">Julia</a>. </div> </div> </div>