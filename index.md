@def title = "Ezgi's Garden"
@def date = Date(2021, 06, 05)


## About me
@@row
@@container
~~~
<img 
    class="left" 
    id=profpic
    src="assets/profile_ezgi.jpg"
>
~~~

Hi, I am Ezgi (she/they). I am a second year **PhD candidate in
Electrical Engineering** at *New York University* under the advisory of
Professor Elza Erkip, [CommIT Group at NYU](https://wp.nyu.edu/elza_erkip/). I
received my (integrated) MEng degree in Electrical Engineering from *Imperial College London* in 2021.

My current interests are in *learned (distributed) compression models* and multi-terminal source coding. My background is in *(information-theoretic) deep learning* and *statistical signal processing*. Outside of academia, I hike as well as try to expand my food and coffee palate.

I am always happy to chat about topics at the intersection of information theory and deep learning -- feel free to drop me an email at *ezgi(dot)ozyilkan(at)nyu(dot)edu*!
@@
@@

## Updates
* May 2023: I presented a poster titled *Neural Distributed Compressor Does Binning* at UC Berkeley Simons Institute's workshop on [*Information-Theoretic Methods for Trustworthy Machine Learning*](https://simons.berkeley.edu/talks/2023-05-24).

* April 2023: *"Learned Wyner--Ziv Compressors Recover Binning"* was accepted to [International Symposium on Information Theory (ISIT) 2023](https://isit2023.org/). Preprint is available [here](https://arxiv.org/abs/2305.04380)!

* December 2022: [*"Learned Disentangled Latent Representations for Scalable Image Coding for Humans and Machines"*](https://arxiv.org/abs/2301.04183) was accepted to [Data Compression Conference (DCC) 2023](https://www.cs.brandeis.edu/~dcc/).

* August 2022: I presented a poster titled *Neural Distributed Source Coding* at [North American School of Information Theory (NASIT) 2022](https://nasit-2022.seas.ucla.edu/poster-symposium-session-assignments/).

* June 2022: Interning at InterDigital -- Emerging Technologies Lab in Los Altos, CA.

