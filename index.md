@def title = "Ezgi's Garden"
@def date = Date(2021, 06, 05)


## About me
@@row
@@container
~~~
<img 
    class="left" 
    id=profpic
    src="assets/pic_ezgi.jpeg"
>
~~~

Hi, I am Ezgi (she/they). I am a **PhD candidate in
Electrical and Computer Engineering** at *New York University* where I'm advised by [Elza Erkip](https://wp.nyu.edu/elza_erkip/). I hold an (integrated) MEng degree in Electrical Electronics Engineering from *Imperial College London*. Recent collaborators include [Jona Ballé](https://balle.io/) and [Aaron B. Wagner](https://www.ece.cornell.edu/faculty-directory/aaron-b-wagner) and [Deniz Gündüz](https://profiles.imperial.ac.uk/d.gunduz). 

I am a collaborative researcher and enjoy working with people from diverse backgrounds. My current research is driven by a passion for **connecting theory and practice** in data compression and communication problems, particularly in distributed scenarios. I leverage tools from **learning, signal processing, compression and information theory**, *yielding interpretable results*. When I am not busy with research, I enjoy hiking and expanding my food and coffee ☕ palate.


I am always happy to chat about topics at the intersection of information theory and deep/machine learning -- feel free to drop me an email at *ezgi(dot)ozyilkan(at)nyu(dot)edu*!

Useful links: [Google Scholar](https://scholar.google.com/citations?hl=en&user=MVZFqdQAAAAJ) | [LinkedIn](https://www.linkedin.com/in/ezgi-ozyilkan-21b2ab191/) | [arXiv](https://arxiv.org/a/ozyilkan_e_1.html) | [GitHub](https://github.com/ezgimez)
@@
@@


## Updates


* February 2025: Excited to be joining **Apple** as an *ML/CV Research Intern* this summer in Cupertino!

* January 2025: Our workshop proposal titled *"Learn to Compress and Compress to Learn"* for [2025 IEEE International Symposium on Information Theory](https://2025.ieee-isit.org/) has been accepted! Check out our workshop website [here](https://learn-to-compress-workshop-isit.github.io/).

* December 2024: Honored to be selected as a recipient of the 2024 [IEEE Signal Processing Society (SPS)](https://signalprocessingsociety.org/) [Scholarship](https://signalprocessingsociety.org/gallery/2024-sps-scholarship-recipients)!

* November 2024: In our recent [NeurIPS'24 workshop paper](https://openreview.net/forum?id=qcM1fkFj3Y), we discuss a few overarching *failure modes* of some popular class of neural compressors -- that is, their difficulty in learning discontinuous functions.  

* October 2024: Our recent preprint titled [*Learning-Based Compress-and-Forward Schemes for the Relay Channel*](https://arxiv.org/abs/2405.09534v1) got accepted to [IEEE Journal on Communications (JSAC)](https://www.comsoc.org/publications/journals/ieee-jsac) and will appear, as part of [this special issue](https://www.comsoc.org/publications/journals/ieee-jsac/cfp/rethinking-information-identification-representation-and), in 2025!

* September 2024: I presented our work titled [*Neural Compress-and-Forward for the Relay Channel*](https://arxiv.org/abs/2404.14594) at [SPAWC 2024](https://spawc2024.org/), in the beautiful Italian city of Lucca! Here is the [poster](assets/Neural_Compress-and-Forward_SPAWC2024_poster.pdf). 

* July 2024: Our [workshop proposal](https://neuralcompression.github.io/workshop24) (compression + machine learning) for NeurIPS 2024 has been accepted! Details will follow shortly :) 


* July 2024: Our recent work titled [*Neural Compress-and-Forward for the Relay Channel*](https://arxiv.org/abs/2404.14594) got accepted to [IEEE International Workshop on Signal Processing Advances in Wireless Communications (SPAWC) 2024](https://spawc2024.org/)!

* May 2024: In our recent preprints ([[1]](https://arxiv.org/abs/2404.14594) and [[2]](https://arxiv.org/abs/2405.09534v1)), we propose neural "compress-and-forward" (CF) schemes for the relay channel, that leverage my previous neural distributed compression work. Our proposed neural CF operates closely to the maximum achievable rate in a "primitive relay channel" and also yields interpretable results :) 

* April 2024: Our recent work titled [*"Neural Distributed Compressor Discovers Binning"*](https://arxiv.org/abs/2310.16961) got accepted to IEEE Journal on Selected Areas in Information Theory (JSAIT), part of the special issue on Toby Berger. 


* April 2024: Our recent [preprint](https://arxiv.org/abs/2403.08411) on robust distributed lossy compression was accepted to the [2024 IEEE International Symposium on Information Theory](https://2024.ieee-isit.org/) Workshops (ISIT'24 Wkshps)!

* March 2024: In our recent [preprint](https://arxiv.org/abs/2403.08411), we extend our [neural distributed lossy compression framework](https://ieeexplore.ieee.org/document/10206542) to more robust/general compression settings -- for example, where side information may be absent. We demonstrate that our learned compressors mimic the theoretical optimum and yield interpretable results :) 

* February 2024: Our recent survey titled *"Distributed Compression in the Era of Machine Learning: A Review of Recent Advances"* will appear [at the Conference on Information Sciences and Systems (CISS'24)](https://ee-ciss.princeton.edu/) as an invited paper! Preprint is available [here](https://arxiv.org/abs/2402.07997).


* January 2024: The full program for our 'Learn to Compress' workshop @ [ISIT'24](https://2024.ieee-isit.org/workshops)  (including keynote speakers and call for papers) is [out](https://learn-to-compress-workshop-isit.github.io/2024/about/).


* December 2023: *"Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information"*  was accepted to the inaugural [2024 IEEE International Conference on Machine Learning for Communication and Networking (ICMLCN)](https://icmlcn2024.ieee-icmlcn.org/)! Preprint is available [here](https://arxiv.org/abs/2310.04311).

* November 2023: Our proposal ["Learn to Compress"](https://2024.ieee-isit.org/workshops) has been accepted as a workshop at [ISIT 2024](https://2024.ieee-isit.org/). The proposal was put forward by [Aaron Wagner (Cornell University)](https://www.ece.cornell.edu/faculty-directory/aaron-b-wagner), [Elza Erkip (NYU)](https://wp.nyu.edu/elza_erkip/) and myself. We will release more details about this workshop in December -- but meanwhile, feel free to check out our [workshop website](https://learn-to-compress-workshop-isit.github.io/2024/about/)!  

* October 2023: The draft version of the journal version of our previous ISIT 2023 paper is available in [arXiv](https://arxiv.org/abs/2310.16961)! We demonstrate that the neural distributed compressor mimics the theoretical optimum for more exemplary sources :) 

* July 2023: I presented our work titled [*"Neural Distributed Compressor Does Binning"*](https://openreview.net/forum?id=3Dq4FZJSga) at Neural Compression Workshop @ ICML'23. Here are the [slides](/assets/Ozyilkan_ICML2023-workshop_final.pdf).


* July 2023: I was selected as the _best reviewer_ for the [Neural Compression Workshop @ ICML'23](https://neuralcompression.github.io/workshop23). 

* July 2023: Our recent ISIT'23 work was accepted as an _oral presentation_ to [Neural Compression Workshop @ ICML'23](https://neuralcompression.github.io/workshop23). 


