@def title = "Ezgi's Garden"
@def date = Date(2021, 06, 05)


## About me
@@row
@@container
~~~
<img 
    class="left" 
    id=profpic
    src="assets/profile_ezgi.jpg"
>
~~~

Hi, I am Ezgi (she/they). I am a third year **PhD candidate in
Electrical Engineering** at *New York University* under the advisory of
Professor Elza Erkip, [CommIT Group at NYU](https://wp.nyu.edu/elza_erkip/). I
received my (integrated) MEng degree in Electrical Engineering from *Imperial College London* in 2021.

My current interests are in (unconventional) **learned compression models**, *quantization* and *representation learning*. My background is in *(information-theoretic) deep learning* and *statistical signal processing*. Outside of academia, I hike as well as try to expand my food and coffee palate.

I am always happy to chat about topics at the intersection of information theory and deep learning -- feel free to drop me an email at *ezgi(dot)ozyilkan(at)nyu(dot)edu*!

Useful links: [Google Scholar](https://scholar.google.com/citations?hl=en&user=MVZFqdQAAAAJ) | [LinkedIn](https://www.linkedin.com/in/ezgi-%C3%B6zy%C4%B1lkan-21b2ab191/) | [arXiv](https://arxiv.org/a/ozyilkan_e_1.html) | [GitHub](https://github.com/ezgimez)
@@
@@

## Updates

* April 2024: Our recent work titled [*"Neural Distributed Compressor Discovers Binning"*](https://arxiv.org/abs/2310.16961) got accepted to IEEE Journal on Selected Areas in Information Theory (JSAIT), part of the special issue on Toby Berger. 


* April 2024: Our recent [preprint](https://arxiv.org/abs/2403.08411) on robust distributed lossy compression was accepted to the [2024 IEEE International Symposium on Information Theory](https://2024.ieee-isit.org/) Workshops (ISIT'24 Wkshps)!

* March 2024: In our recent [preprint](https://arxiv.org/abs/2403.08411), we extend our [neural distributed lossy compression framework](https://ieeexplore.ieee.org/document/10206542) to more robust/general compression settings -- for example, where side information may be absent. We demonstrate that our learned compressors mimic the theoretical optimum and yield interpretable results :) 

* February 2024: Our recent survey titled *"Distributed Compression in the Era of Machine Learning: A Review of Recent Advances"* will appear [at the Conference on Information Sciences and Systems (CISS'24)](https://ee-ciss.princeton.edu/) as an invited paper! Preprint is available [here](https://arxiv.org/abs/2402.07997).


* January 2024: The full program for our 'Learn to Compress' workshop @ [ISIT'24](https://2024.ieee-isit.org/workshops)  (including keynote speakers and call for papers) is [out](https://learn-to-compress-workshop-isit.github.io/).


* December 2023: *"Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information"*  was accepted to the inaugural [2024 IEEE International Conference on Machine Learning for Communication and Networking (ICMLCN)](https://icmlcn2024.ieee-icmlcn.org/)! Preprint is available [here](https://arxiv.org/abs/2310.04311).

* November 2023: Our proposal ["Learn to Compress"](https://2024.ieee-isit.org/workshops) has been accepted as a workshop at [ISIT 2024](https://2024.ieee-isit.org/). The proposal was put forward by [Aaron Wagner (Cornell University)](https://www.ece.cornell.edu/faculty-directory/aaron-b-wagner), [Elza Erkip (NYU)](https://wp.nyu.edu/elza_erkip/) and myself. We will release more details about this workshop in December -- but meanwhile, feel free to check out our [workshop website](https://learn-to-compress-workshop-isit.github.io/)!  

* October 2023: The draft version of the journal version of our previous ISIT 2023 paper is available in [arXiv](https://arxiv.org/abs/2310.16961)! We demonstrate that the neural distributed compressor mimics the theoretical optimum for more exemplary sources :) 

* July 2023: I presented our work titled [*"Neural Distributed Compressor Does Binning"*](https://openreview.net/forum?id=3Dq4FZJSga) at Neural Compression Workshop @ ICML'23. Here are the [slides](/assets/Ozyilkan_ICML2023-workshop_final.pdf).


* July 2023: I was selected as the _best reviewer_ for the [Neural Compression Workshop @ ICML'23](https://neuralcompression.github.io/workshop23). 

* July 2023: Our recent ISIT'23 work was accepted as an _oral presentation_ to [Neural Compression Workshop @ ICML'23](https://neuralcompression.github.io/workshop23). 

* June 2023: I presented our work titled *"Learned Wyner--Ziv Compressors Recover Binning"* at [International Symposium on Information Theory (ISIT) 2023](https://isit2023.org/). Here are the [slides](/assets/Ozyilkan_ISIT2023_final.pdf)!

* June 2023: I presented a poster about our upcoming [ISIT'23 paper](https://arxiv.org/abs/2305.04380) at [North American School of Information Theory (NASIT) 2023](https://nasit.seas.upenn.edu/home).


* May 2023: I presented a [poster](/assets/Ozyilkan_Simons-Institute_Poster_May2023.pdf) titled *Neural Distributed Compressor Does Binning* at UC Berkeley Simons Institute's workshop on [*Information-Theoretic Methods for Trustworthy Machine Learning*](https://simons.berkeley.edu/talks/2023-05-24).

* April 2023: *"Learned Wyner--Ziv Compressors Recover Binning"* was accepted to [International Symposium on Information Theory (ISIT) 2023](https://isit2023.org/). Preprint is available [here](https://arxiv.org/abs/2305.04380)!

* December 2022: [*"Learned Disentangled Latent Representations for Scalable Image Coding for Humans and Machines"*](https://arxiv.org/abs/2301.04183) was accepted to [Data Compression Conference (DCC) 2023](https://www.cs.brandeis.edu/~dcc/).

* August 2022: I presented a poster titled *Neural Distributed Source Coding* at [North American School of Information Theory (NASIT) 2022](https://nasit-2022.seas.ucla.edu/poster-symposium-session-assignments/).

* June 2022: Interning at InterDigital -- Emerging Technologies Lab in Los Altos, CA.

